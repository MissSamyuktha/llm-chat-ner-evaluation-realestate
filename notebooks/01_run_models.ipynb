{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from chats import chats\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ac363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# loading api key\n",
    "load_dotenv(override=True)\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751ecf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "\n",
    "# For Groq, we can use the OpenAI python client\n",
    "# Because they have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b846bda",
   "metadata": {},
   "source": [
    "# Model Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff862e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(prompt_template, model_name):\n",
    "    \"\"\"\n",
    "    Generate model responses for a set of chats.\n",
    "\n",
    "    Each chat text from the global `chats` dictionary is inserted into\n",
    "    the given prompt template and sent to the specified model via Groq.\n",
    "    The function collects the model's replies in a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_template : str\n",
    "        Template string with a `{chat_text}` placeholder.\n",
    "    model_name : str\n",
    "        Name of the model to query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping of chat IDs to the model's response text.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    for chat_id, chat_text in chats.items():\n",
    "        prompt = prompt_template.format(chat_text=chat_text)\n",
    "\n",
    "        response = groq.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        outputs[chat_id] = response.choices[0].message.content\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1cbf36",
   "metadata": {},
   "source": [
    "# Prompt Strategy 1 - Naive Baseline\n",
    "\n",
    "Purpose: establish a weak baseline  \n",
    "Expected: errors, hallucinations\n",
    "\n",
    "Likely failures:\n",
    "- Infers missing data\n",
    "- Wrong date conversions\n",
    "- Extra keys\n",
    "\n",
    "We Use this to understand why prompting matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce81cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_1 = \"\"\"\n",
    "Extract the required entities from the following chat conversation and return them as JSON.\n",
    "\n",
    "Chat:\n",
    "{chat_text}\n",
    "\n",
    "Output the JSON only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama model with prompt1\n",
    "llama_prompt1_output = run_model(\n",
    "    prompt_template=PROMPT_1,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/llama_prompt1_raw.json\", \"w\") as f:\n",
    "    json.dump(llama_prompt1_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt model with prompt1\n",
    "gpt_prompt1_output = run_model(\n",
    "    prompt_template=PROMPT_1,\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a52aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/gpt_prompt1_raw.json\", \"w\") as f:\n",
    "    json.dump(gpt_prompt1_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ffebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen model with prompt1\n",
    "qwen_prompt1_output = run_model(\n",
    "    prompt_template=PROMPT_1,\n",
    "    model_name=\"qwen/qwen3-32b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed79a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/qwen_prompt1_raw.json\", \"w\") as f:\n",
    "    json.dump(qwen_prompt1_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d96fe",
   "metadata": {},
   "source": [
    "# Prompt Strategy 2 - Schema Strict\n",
    "\n",
    "Purpose: enforce discipline  \n",
    "Expected: best overall F1\n",
    "\n",
    "Strength:\n",
    "- Lowest hallucination\n",
    "- Clean JSON\n",
    "\n",
    "Weakness:\n",
    "- Might miss borderline mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b497e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_2 = \"\"\"\n",
    "You are given a chat conversation between a visitor and a real estate agent.\n",
    "\n",
    "Your task is to extract the following entities strictly according to the rules.\n",
    "\n",
    "Rules:\n",
    "- Capture an entity ONLY if it is explicitly mentioned\n",
    "- Do NOT infer or guess missing information\n",
    "- Do NOT infer names from email addresses\n",
    "- If an entity is not mentioned, return null\n",
    "- Return EXACTLY the JSON schema provided\n",
    "- Do NOT add any extra keys\n",
    "- Do NOT include explanations or reasoning\n",
    "\n",
    "Chat:\n",
    "{chat_text}\n",
    "\n",
    "Output JSON schema:\n",
    "{{\n",
    "  \"first_name\": null,\n",
    "  \"last_name\": null,\n",
    "  \"phone_number\": null,\n",
    "  \"email\": null,\n",
    "  \"budget\": null,\n",
    "  \"current_location\": null,\n",
    "  \"preferred_location\": null,\n",
    "  \"profession\": null,\n",
    "  \"date_of_visit\": null,\n",
    "  \"buying_timeline_weeks\": null\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c162807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama model with prompt2\n",
    "llama_prompt2_output = run_model(\n",
    "    prompt_template=PROMPT_2,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/llama_prompt2_raw.json\", \"w\") as f:\n",
    "    json.dump(llama_prompt2_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt model with prompt2\n",
    "gpt_prompt2_output = run_model(\n",
    "    prompt_template=PROMPT_2,\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/gpt_prompt2_raw.json\", \"w\") as f:\n",
    "    json.dump(gpt_prompt2_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d99af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen model with prompt2\n",
    "qwen_prompt2_output = run_model(\n",
    "    prompt_template=PROMPT_2,\n",
    "    model_name=\"qwen/qwen3-32b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3119304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/qwen_prompt2_raw.json\", \"w\") as f:\n",
    "    json.dump(qwen_prompt2_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89c3ec",
   "metadata": {},
   "source": [
    "# Prompt Strategy 3 — Hidden Chain-of-Thought\n",
    "\n",
    "Purpose: improve reasoning without leaking explanations  \n",
    "Expected: higher accuracy on dates & timelines\n",
    "\n",
    "Strength:\n",
    "- Better date normalization\n",
    "- Better timeline conversion\n",
    "\n",
    "Risk:\n",
    "- Some models still leak text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7132a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_3 = \"\"\"\n",
    "You are given a chat conversation between a visitor and a real estate agent.\n",
    "\n",
    "Think step by step to identify ONLY explicitly mentioned entities and normalize them according to the rules.\n",
    "Do not include your reasoning in the final output.\n",
    "\n",
    "Rules:\n",
    "- Do not infer missing information\n",
    "- Return null if an entity is not explicitly stated\n",
    "- Return exactly the JSON schema provided\n",
    "\n",
    "Chat:\n",
    "{chat_text}\n",
    "\n",
    "Final Output (JSON only):\n",
    "{{\n",
    "  \"first_name\": null,\n",
    "  \"last_name\": null,\n",
    "  \"phone_number\": null,\n",
    "  \"email\": null,\n",
    "  \"budget\": null,\n",
    "  \"current_location\": null,\n",
    "  \"preferred_location\": null,\n",
    "  \"profession\": null,\n",
    "  \"date_of_visit\": null,\n",
    "  \"buying_timeline_weeks\": null\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20739b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama model with prompt3\n",
    "llama_prompt3_output = run_model(\n",
    "    prompt_template=PROMPT_3,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8905622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/llama_prompt3_raw.json\", \"w\") as f:\n",
    "    json.dump(llama_prompt3_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6918bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt model with prompt3\n",
    "gpt_prompt3_output = run_model(\n",
    "    prompt_template=PROMPT_3,\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ca4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/gpt_prompt3_raw.json\", \"w\") as f:\n",
    "    json.dump(gpt_prompt3_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ca06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen model with prompt3\n",
    "qwen_prompt3_output = run_model(\n",
    "    prompt_template=PROMPT_3,\n",
    "    model_name=\"qwen/qwen3-32b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e818ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/qwen_prompt3_raw.json\", \"w\") as f:\n",
    "    json.dump(qwen_prompt3_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71874a5",
   "metadata": {},
   "source": [
    "# Prompt Strategy 4 — Negative Instruction Prompt\n",
    "\n",
    "Purpose: reduce hallucinations  \n",
    "Expected: higher precision, lower recall\n",
    "\n",
    "Strength:\n",
    "- Excellent at suppressing hallucinations\n",
    "\n",
    "Weakness:\n",
    "- Misses borderline but valid mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "378f7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_4 = \"\"\"\n",
    "Extract entities from the chat below.\n",
    "\n",
    "Important constraints:\n",
    "- Do NOT infer missing information\n",
    "- Do NOT guess professions\n",
    "- Do NOT extract names from email addresses\n",
    "- Do NOT fabricate phone numbers or dates\n",
    "- If an entity is not explicitly mentioned, return null\n",
    "\n",
    "Chat:\n",
    "{chat_text}\n",
    "\n",
    "Return ONLY the JSON in the following format:\n",
    "{{\n",
    "  \"first_name\": null,\n",
    "  \"last_name\": null,\n",
    "  \"phone_number\": null,\n",
    "  \"email\": null,\n",
    "  \"budget\": null,\n",
    "  \"current_location\": null,\n",
    "  \"preferred_location\": null,\n",
    "  \"profession\": null,\n",
    "  \"date_of_visit\": null,\n",
    "  \"buying_timeline_weeks\": null\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc82f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama model with prompt4\n",
    "llama_prompt4_output = run_model(\n",
    "    prompt_template=PROMPT_4,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17089a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/llama_prompt4_raw.json\", \"w\") as f:\n",
    "    json.dump(llama_prompt4_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5121911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt model with prompt4\n",
    "gpt_prompt4_output = run_model(\n",
    "    prompt_template=PROMPT_4,\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/gpt_prompt4_raw.json\", \"w\") as f:\n",
    "    json.dump(gpt_prompt4_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf662ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen model with prompt4\n",
    "qwen_prompt4_output = run_model(\n",
    "    prompt_template=PROMPT_4,\n",
    "    model_name=\"qwen/qwen3-32b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/qwen_prompt4_raw.json\", \"w\") as f:\n",
    "    json.dump(qwen_prompt4_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0790f",
   "metadata": {},
   "source": [
    "# Prompt Strategy 5 — One-Shot Example\n",
    "\n",
    "Purpose: teach behavior by example  \n",
    "Expected: better consistency across chats\n",
    "\n",
    "Strength:\n",
    "- Improves structure for weaker models\n",
    "\n",
    "Weakness:\n",
    "- Overfits example sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e162a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_5 = \"\"\"\n",
    "Example:\n",
    "\n",
    "Chat:\n",
    "Visitor: Hi, my name is Ramesh Kumar. I want to buy a flat in Andheri.\n",
    "Agent: May I know your budget?\n",
    "Visitor: Around 1.2 crore. I stay in Borivali.\n",
    "\n",
    "Output:\n",
    "{{\n",
    "  \"first_name\": \"Ramesh\",\n",
    "  \"last_name\": \"Kumar\",\n",
    "  \"phone_number\": null,\n",
    "  \"email\": null,\n",
    "  \"budget\": 12000000,\n",
    "  \"current_location\": \"Borivali\",\n",
    "  \"preferred_location\": \"Andheri\",\n",
    "  \"profession\": null,\n",
    "  \"date_of_visit\": null,\n",
    "  \"buying_timeline_weeks\": null\n",
    "}}\n",
    "\n",
    "Now extract entities from the following chat.\n",
    "\n",
    "Chat:\n",
    "{chat_text}\n",
    "\n",
    "Return JSON only using the same schema.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama model with prompt5\n",
    "llama_prompt5_output = run_model(\n",
    "    prompt_template=PROMPT_5,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68961b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/llama_prompt5_raw.json\", \"w\") as f:\n",
    "    json.dump(llama_prompt5_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt model with prompt5\n",
    "gpt_prompt5_output = run_model(\n",
    "    prompt_template=PROMPT_5,\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50118df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/gpt_prompt5_raw.json\", \"w\") as f:\n",
    "    json.dump(gpt_prompt5_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721532d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen model with prompt5\n",
    "qwen_prompt5_output = run_model(\n",
    "    prompt_template=PROMPT_5,\n",
    "    model_name=\"qwen/qwen3-32b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0237e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/qwen_prompt5_raw.json\", \"w\") as f:\n",
    "    json.dump(qwen_prompt5_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d9be0",
   "metadata": {},
   "source": [
    "# Prompt Strategy 6 — Entity-by-Entity Extraction\n",
    "\n",
    "Purpose: reduce cross-entity confusion  \n",
    "Expected: better precision on names & contacts\n",
    "\n",
    "Strength:\n",
    "- Helps smaller models\n",
    "\n",
    "Weakness:\n",
    "- Slightly verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd637e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_6 = \"\"\"\n",
    "Extract the following entities from the chat:\n",
    "\n",
    "1. First Name\n",
    "2. Last Name\n",
    "3. Phone Number\n",
    "4. Email\n",
    "5. Budget (integer INR)\n",
    "6. Current Location\n",
    "7. Preferred Location\n",
    "8. Profession (service, business, retired)\n",
    "9. Date of Visit (YYYY-MM-DD)\n",
    "10. Buying Timeline in Weeks\n",
    "\n",
    "Rules:\n",
    "- Extract ONLY if explicitly mentioned\n",
    "- Do NOT infer missing values\n",
    "- Return null for missing entities\n",
    "\n",
    "Chat:\n",
    "{chat_text}\n",
    "\n",
    "Return the result strictly as JSON using this schema:\n",
    "{{\n",
    "  \"first_name\": null,\n",
    "  \"last_name\": null,\n",
    "  \"phone_number\": null,\n",
    "  \"email\": null,\n",
    "  \"budget\": null,\n",
    "  \"current_location\": null,\n",
    "  \"preferred_location\": null,\n",
    "  \"profession\": null,\n",
    "  \"date_of_visit\": null,\n",
    "  \"buying_timeline_weeks\": null\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama model with prompt6\n",
    "llama_prompt6_output = run_model(\n",
    "    prompt_template=PROMPT_6,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/llama_prompt6_raw.json\", \"w\") as f:\n",
    "    json.dump(llama_prompt6_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1644efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt model with prompt6\n",
    "gpt_prompt6_output = run_model(\n",
    "    prompt_template=PROMPT_6,\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/gpt_prompt6_raw.json\", \"w\") as f:\n",
    "    json.dump(gpt_prompt6_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9380513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen model with prompt6\n",
    "qwen_prompt6_output = run_model(\n",
    "    prompt_template=PROMPT_6,\n",
    "    model_name=\"qwen/qwen3-32b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing raw json file outputed by the model\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/qwen_prompt6_raw.json\", \"w\") as f:\n",
    "    json.dump(qwen_prompt6_output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d6197",
   "metadata": {},
   "source": [
    "# Modifying model call function to capture latency and token usage\n",
    "## Add timing + usage capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c7007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_model_with_metrics(prompt_template, model_name):\n",
    "    \"\"\"\n",
    "    Run a model on chat inputs and collect performance metrics.\n",
    "\n",
    "    Each chat text from the global `chats` dictionary is formatted into\n",
    "    the given prompt template and sent to the specified model via Groq.\n",
    "    The function records the model's responses along with latency and\n",
    "    token usage statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_template : str\n",
    "        Template string with a `{chat_text}` placeholder.\n",
    "    model_name : str\n",
    "        Name of the model to query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (outputs, metrics) where:\n",
    "        - outputs (dict): Mapping of chat IDs to model response text.\n",
    "        - metrics (dict): Contains:\n",
    "            * avg_latency_sec (float): Average response time in seconds.\n",
    "            * latencies (list of float): Per-chat response times.\n",
    "            * token_usages (list of dict): Token usage details if available.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    latencies = []\n",
    "    token_usages = []\n",
    "\n",
    "    for chat_id, chat_text in chats.items():\n",
    "        prompt = prompt_template.format(chat_text=chat_text)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = groq.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Latency\n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "        # Output\n",
    "        outputs[chat_id] = response.choices[0].message.content\n",
    "\n",
    "        # Token usage (if exposed)\n",
    "        if hasattr(response, \"usage\") and response.usage is not None:\n",
    "            token_usages.append({\n",
    "                \"prompt_tokens\": getattr(response.usage, \"prompt_tokens\", None),\n",
    "                \"completion_tokens\": getattr(response.usage, \"completion_tokens\", None),\n",
    "                \"total_tokens\": getattr(response.usage, \"total_tokens\", None),\n",
    "            })\n",
    "\n",
    "    metrics = {\n",
    "        \"avg_latency_sec\": sum(latencies) / len(latencies),\n",
    "        \"latencies\": latencies,\n",
    "        \"token_usages\": token_usages\n",
    "    }\n",
    "\n",
    "    return outputs, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1c573",
   "metadata": {},
   "source": [
    "## Running modified function call on Top 3 configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7177cb",
   "metadata": {},
   "source": [
    "## GPT-OSS model call with prompt_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, metrics = run_model_with_metrics(\n",
    "    prompt_template=PROMPT_6,\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")\n",
    "\n",
    "# storing metrics file outputed by the model\n",
    "with open(\"outputs/gpt_prompt6_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302dd7cc",
   "metadata": {},
   "source": [
    "Average latency of GPT-OSS is 3.01 seconds.  \n",
    "Prompt tokens are in the range of 400-600.  \n",
    "Total token count falls in the range 730-1140."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e9889",
   "metadata": {},
   "source": [
    "## Llama model call with prompt_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96738fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, metrics = run_model_with_metrics(\n",
    "    prompt_template=PROMPT_6,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "# storing metrics file outputed by the model\n",
    "with open(\"outputs/llama_prompt6_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6d159",
   "metadata": {},
   "source": [
    "Average latency of Llama is 1.8 seconds.  \n",
    "Prompt tokens are in the range of 400-600.  \n",
    "Total token count falls in the range 475-720."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9780a",
   "metadata": {},
   "source": [
    "## Qwen model call with prompt_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733862b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, metrics = run_model_with_metrics(\n",
    "    prompt_template=PROMPT_6,\n",
    "    model_name=\"qwen/qwen3-32b\"\n",
    ")\n",
    "\n",
    "# storing metrics file outputed by the model\n",
    "with open(\"outputs/qwen_prompt6_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21904db9",
   "metadata": {},
   "source": [
    "Average latency of Qwen is 5.5 seconds.  \n",
    "Prompt tokens are in the range of 400-600.  \n",
    "Total token count falls in the range 700-1600."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
