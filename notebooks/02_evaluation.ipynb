{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbd9a0",
   "metadata": {},
   "source": [
    "# Load gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1015372",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\kaliv\\\\SamProjects\\\\CommVersion_Assignment\\\\data\\\\gold_labels.json\") as f:\n",
    "    gold = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c965",
   "metadata": {},
   "source": [
    "# Discover all output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a24e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = [\n",
    "    f for f in os.listdir(\"outputs\")\n",
    "    if f.endswith(\".json\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c48d8",
   "metadata": {},
   "source": [
    "# Safe JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def safe_parse_json(text):\n",
    "    \"\"\"\n",
    "    Safely attempt to parse a JSON string.\n",
    "\n",
    "    The function first tries to directly parse the given text as JSON.\n",
    "    If that fails, it searches for the first JSON-like block (enclosed\n",
    "    in curly braces) within the text and attempts to parse that instead.\n",
    "    Returns None if parsing is unsuccessful.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str or None\n",
    "        The input string that may contain JSON data. If None, the function\n",
    "        immediately returns None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict or list or None\n",
    "        Parsed JSON object (dict or list) if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    # Try direct parse\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try extracting JSON from code blocks\n",
    "    match = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78986e",
   "metadata": {},
   "source": [
    "# Parse & normalize all outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf302347",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parsed_outputs = {}\n",
    "\n",
    "for file_name in output_files:\n",
    "    with open(f\"outputs/{file_name}\") as f:\n",
    "        raw_outputs = json.load(f)\n",
    "\n",
    "    parsed_outputs = {}\n",
    "\n",
    "    for chat_id, raw_text in raw_outputs.items():\n",
    "        parsed_outputs[chat_id] = safe_parse_json(raw_text)\n",
    "\n",
    "    all_parsed_outputs[file_name] = parsed_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af9715",
   "metadata": {},
   "source": [
    "## sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d67463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_prompt1_raw.json 20 / 20 valid JSON\n",
      "gpt_prompt2_raw.json 20 / 20 valid JSON\n",
      "gpt_prompt3_raw.json 20 / 20 valid JSON\n",
      "gpt_prompt4_raw.json 20 / 20 valid JSON\n",
      "gpt_prompt5_raw.json 20 / 20 valid JSON\n",
      "gpt_prompt6_raw.json 20 / 20 valid JSON\n",
      "llama_prompt1_raw.json 20 / 20 valid JSON\n",
      "llama_prompt2_raw.json 20 / 20 valid JSON\n",
      "llama_prompt3_raw.json 20 / 20 valid JSON\n",
      "llama_prompt4_raw.json 20 / 20 valid JSON\n",
      "llama_prompt5_raw.json 20 / 20 valid JSON\n",
      "llama_prompt6_raw.json 20 / 20 valid JSON\n",
      "qwen_prompt1_raw.json 20 / 20 valid JSON\n",
      "qwen_prompt2_raw.json 17 / 20 valid JSON\n",
      "qwen_prompt3_raw.json 19 / 20 valid JSON\n",
      "qwen_prompt4_raw.json 16 / 20 valid JSON\n",
      "qwen_prompt5_raw.json 14 / 20 valid JSON\n",
      "qwen_prompt6_raw.json 20 / 20 valid JSON\n"
     ]
    }
   ],
   "source": [
    "for file, chats in all_parsed_outputs.items():\n",
    "    valid = sum(v is not None for v in chats.values())\n",
    "    print(file, valid, \"/ 20 valid JSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaeaec2",
   "metadata": {},
   "source": [
    "- GPT: perfect JSON reliability across all prompts\n",
    "- LLaMA: perfect JSON reliability after parsing cleanup\n",
    "- Qwen: sensitive to prompt strategy (drops in strict / negative prompts)\n",
    "\n",
    "ðŸ‘‰ Some models are more prompt-sensitive in structured extraction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3a9fe",
   "metadata": {},
   "source": [
    "# PHASE 3B â€” Entity-level Metrics (Precision / Recall / F1)\n",
    "\n",
    "We now answer the core question:\n",
    "\n",
    "When JSON is valid, how accurate is the extraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71304e39",
   "metadata": {},
   "source": [
    "## Define entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3aadd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES = [\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"phone_number\",\n",
    "    \"email\",\n",
    "    \"budget\",\n",
    "    \"current_location\",\n",
    "    \"preferred_location\",\n",
    "    \"profession\",\n",
    "    \"date_of_visit\",\n",
    "    \"buying_timeline_weeks\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705a89f",
   "metadata": {},
   "source": [
    "## Comparison logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75191299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entity(pred, truth):\n",
    "    \"\"\"\n",
    "    Compare predicted and true entity values.\n",
    "\n",
    "    This function evaluates a prediction against the ground truth and\n",
    "    returns a classification code:\n",
    "      - \"TP\" (True Positive): prediction matches the truth.\n",
    "      - \"TN\" (True Negative): both prediction and truth are None.\n",
    "      - \"FP\" (False Positive): prediction exists but truth is None.\n",
    "      - \"FN\" (False Negative): prediction is None but truth exists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : any or None\n",
    "        The predicted entity value.\n",
    "    truth : any or None\n",
    "        The ground truth entity value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        One of {\"TP\", \"TN\", \"FP\", \"FN\"} indicating the comparison result.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pred is None and truth is None:\n",
    "        return \"TN\"\n",
    "    if pred == truth:\n",
    "        return \"TP\"\n",
    "    if pred is not None and truth is None:\n",
    "        return \"FP\"\n",
    "    if pred is None and truth is not None:\n",
    "        return \"FN\"\n",
    "    return \"FN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5daed1c",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc298ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(parsed_outputs, gold):\n",
    "    \"\"\"\n",
    "    Evaluate predicted outputs against gold-standard labels.\n",
    "\n",
    "    For each chat and entity, the function compares the predicted value\n",
    "    with the ground truth using `compare_entity`. It aggregates counts\n",
    "    of \"TP\", \"TN\", \"FP\", and \"FN\" for each entity type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parsed_outputs : dict\n",
    "        Mapping of chat IDs to parsed model outputs (dicts of entity values).\n",
    "    gold : dict\n",
    "        Mapping of chat IDs to gold-standard entity labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each entity maps to a Counter object summarizing\n",
    "        the number of TP, TN, FP, and FN results.\n",
    "    \"\"\"\n",
    "\n",
    "    counts = {e: Counter() for e in ENTITIES}\n",
    "\n",
    "    for chat_id in gold:\n",
    "        for entity in ENTITIES:\n",
    "            truth = gold[chat_id][entity]\n",
    "\n",
    "            if parsed_outputs[chat_id] is None:\n",
    "                pred = None\n",
    "            else:\n",
    "                pred = parsed_outputs[chat_id].get(entity)\n",
    "\n",
    "            result = compare_entity(pred, truth)\n",
    "            counts[entity][result] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f4338",
   "metadata": {},
   "source": [
    "## Run evaluation for all 18 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "916b7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "\n",
    "for file_name, parsed_outputs in all_parsed_outputs.items():\n",
    "    metrics = evaluate_file(parsed_outputs, gold)\n",
    "    all_metrics[file_name] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23802002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': Counter({'TP': 18, 'TN': 2}),\n",
       " 'last_name': Counter({'TP': 13, 'TN': 7}),\n",
       " 'phone_number': Counter({'FN': 11, 'TN': 9}),\n",
       " 'email': Counter({'TP': 12, 'TN': 8}),\n",
       " 'budget': Counter({'FN': 19, 'FP': 1}),\n",
       " 'current_location': Counter({'TP': 20}),\n",
       " 'preferred_location': Counter({'TP': 20}),\n",
       " 'profession': Counter({'FN': 10, 'TN': 5, 'FP': 3, 'TP': 2}),\n",
       " 'date_of_visit': Counter({'TP': 12, 'TN': 4, 'FN': 4}),\n",
       " 'buying_timeline_weeks': Counter({'FN': 12, 'TP': 7, 'TN': 1})}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look on all metrics of a model-prompt output\n",
    "all_metrics['gpt_prompt2_raw.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47203060",
   "metadata": {},
   "source": [
    "## Compute Precision / Recall / F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbeed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prf(counter):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score from classification counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    counter : collections.Counter or dict\n",
    "        A mapping containing counts for \"TP\", \"FP\", and \"FN\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of float\n",
    "        (precision, recall, f1) where:\n",
    "        - precision = TP / (TP + FP), 0 if denominator is 0\n",
    "        - recall    = TP / (TP + FN), 0 if denominator is 0\n",
    "        - f1        = harmonic mean of precision and recall, 0 if denominator is 0\n",
    "    \"\"\"\n",
    "\n",
    "    tp = counter[\"TP\"]\n",
    "    fp = counter[\"FP\"]\n",
    "    fn = counter[\"FN\"]\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777a86d",
   "metadata": {},
   "source": [
    "## Aggregate per file (OVERALL score)\n",
    "\n",
    "Use micro-average (best choice here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_micro_f1(metrics):\n",
    "    \"\"\"\n",
    "    Compute micro-averaged precision, recall, and F1 score across all entities.\n",
    "\n",
    "    The function aggregates true positives, false positives, and false negatives\n",
    "    from the provided metrics dictionary and calculates overall precision,\n",
    "    recall, and F1 using micro-averaging.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics : dict\n",
    "        A dictionary mapping each entity to a Counter or dict containing\n",
    "        counts for \"TP\", \"FP\", and \"FN\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of float\n",
    "        (precision, recall, f1) representing the micro-averaged scores.\n",
    "    \"\"\"\n",
    "\n",
    "    tp = fp = fn = 0\n",
    "\n",
    "    for entity in ENTITIES:\n",
    "        tp += metrics[entity][\"TP\"]\n",
    "        fp += metrics[entity][\"FP\"]\n",
    "        fn += metrics[entity][\"FN\"]\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf21d5a",
   "metadata": {},
   "source": [
    "## Create a summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13ea6423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_prompt</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt_prompt6_raw.json</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.83750</td>\n",
       "      <td>0.908475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama_prompt6_raw.json</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.81875</td>\n",
       "      <td>0.891156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>qwen_prompt6_raw.json</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.81250</td>\n",
       "      <td>0.890411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt_prompt5_raw.json</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.79375</td>\n",
       "      <td>0.875862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama_prompt5_raw.json</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.78750</td>\n",
       "      <td>0.868966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt_prompt3_raw.json</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.66250</td>\n",
       "      <td>0.785185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama_prompt3_raw.json</td>\n",
       "      <td>0.963303</td>\n",
       "      <td>0.65625</td>\n",
       "      <td>0.780669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_prompt2_raw.json</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.65000</td>\n",
       "      <td>0.776119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt_prompt4_raw.json</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.63750</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_prompt2_raw.json</td>\n",
       "      <td>0.962617</td>\n",
       "      <td>0.64375</td>\n",
       "      <td>0.771536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>qwen_prompt3_raw.json</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.63125</td>\n",
       "      <td>0.765152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama_prompt4_raw.json</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.61250</td>\n",
       "      <td>0.748092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen_prompt2_raw.json</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.56875</td>\n",
       "      <td>0.716535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>qwen_prompt5_raw.json</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.56875</td>\n",
       "      <td>0.716535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>qwen_prompt4_raw.json</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.55000</td>\n",
       "      <td>0.706827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_prompt1_raw.json</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.18125</td>\n",
       "      <td>0.303665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama_prompt1_raw.json</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.13750</td>\n",
       "      <td>0.239130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qwen_prompt1_raw.json</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.11875</td>\n",
       "      <td>0.209945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_prompt  precision   recall        f1\n",
       "5     gpt_prompt6_raw.json   0.992593  0.83750  0.908475\n",
       "11  llama_prompt6_raw.json   0.977612  0.81875  0.891156\n",
       "17   qwen_prompt6_raw.json   0.984848  0.81250  0.890411\n",
       "4     gpt_prompt5_raw.json   0.976923  0.79375  0.875862\n",
       "10  llama_prompt5_raw.json   0.969231  0.78750  0.868966\n",
       "2     gpt_prompt3_raw.json   0.963636  0.66250  0.785185\n",
       "8   llama_prompt3_raw.json   0.963303  0.65625  0.780669\n",
       "1     gpt_prompt2_raw.json   0.962963  0.65000  0.776119\n",
       "3     gpt_prompt4_raw.json   0.980769  0.63750  0.772727\n",
       "7   llama_prompt2_raw.json   0.962617  0.64375  0.771536\n",
       "14   qwen_prompt3_raw.json   0.971154  0.63125  0.765152\n",
       "9   llama_prompt4_raw.json   0.960784  0.61250  0.748092\n",
       "13   qwen_prompt2_raw.json   0.968085  0.56875  0.716535\n",
       "16   qwen_prompt5_raw.json   0.968085  0.56875  0.716535\n",
       "15   qwen_prompt4_raw.json   0.988764  0.55000  0.706827\n",
       "0     gpt_prompt1_raw.json   0.935484  0.18125  0.303665\n",
       "6   llama_prompt1_raw.json   0.916667  0.13750  0.239130\n",
       "12   qwen_prompt1_raw.json   0.904762  0.11875  0.209945"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for file_name, metrics in all_metrics.items():\n",
    "    p, r, f1 = overall_micro_f1(metrics)\n",
    "    rows.append({\n",
    "        \"model_prompt\": file_name,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b1563",
   "metadata": {},
   "source": [
    "Top performers (overall micro-F1)\n",
    "\n",
    "1. GPT + Prompt 6 : F1 = 0.908  \n",
    "2. LLaMA + Prompt 6 : F1 = 0.891  \n",
    "3. Qwen + Prompt 6 : F1 = 0.890  \n",
    "\n",
    "ðŸ‘‰ Prompt Strategy 6 (Entity-by-Entity extraction) is the winner across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40011d02",
   "metadata": {},
   "source": [
    "## Entity-wise table of top 3 configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_wise_df(metrics, label):\n",
    "    \"\"\"\n",
    "    Build a DataFrame of precision, recall, and F1 scores for each entity.\n",
    "\n",
    "    For every entity in `ENTITIES`, the function computes precision, recall,\n",
    "    and F1 using `compute_prf` and organizes the results into a pandas\n",
    "    DataFrame. Each row corresponds to one entity and includes the model\n",
    "    label for reference.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics : dict\n",
    "        A dictionary mapping each entity to a Counter or dict with counts\n",
    "        for \"TP\", \"FP\", and \"FN\".\n",
    "    label : str\n",
    "        Identifier for the model or prompt, stored in the \"model_prompt\" column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with columns:\n",
    "        - model_prompt (str)\n",
    "        - entity (str)\n",
    "        - precision (float)\n",
    "        - recall (float)\n",
    "        - f1 (float)\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    for entity in ENTITIES:\n",
    "        p, r, f1 = compute_prf(metrics[entity])\n",
    "        rows.append({\n",
    "            \"model_prompt\": label,\n",
    "            \"entity\": entity,\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650546bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_prompt</th>\n",
       "      <th>entity</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>first_name</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>last_name</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>phone_number</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>email</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>budget</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>current_location</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>preferred_location</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>profession</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>date_of_visit</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GPT + Prompt 6</td>\n",
       "      <td>buying_timeline_weeks</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.592593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_prompt                 entity  precision    recall        f1\n",
       "0  GPT + Prompt 6             first_name   1.000000  1.000000  1.000000\n",
       "1  GPT + Prompt 6              last_name   1.000000  1.000000  1.000000\n",
       "2  GPT + Prompt 6           phone_number   0.000000  0.000000  0.000000\n",
       "3  GPT + Prompt 6                  email   1.000000  1.000000  1.000000\n",
       "4  GPT + Prompt 6                 budget   1.000000  0.894737  0.944444\n",
       "5  GPT + Prompt 6       current_location   1.000000  1.000000  1.000000\n",
       "6  GPT + Prompt 6     preferred_location   1.000000  1.000000  1.000000\n",
       "7  GPT + Prompt 6             profession   0.916667  0.916667  0.916667\n",
       "8  GPT + Prompt 6          date_of_visit   1.000000  0.937500  0.967742\n",
       "9  GPT + Prompt 6  buying_timeline_weeks   1.000000  0.421053  0.592593"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_prompt6_entities_df = pd.concat([\n",
    "    entity_wise_df(all_metrics[\"gpt_prompt6_raw.json\"], \"GPT + Prompt 6\")\n",
    "])\n",
    "\n",
    "gpt_prompt6_entities_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d392bbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_prompt</th>\n",
       "      <th>entity</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>first_name</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>last_name</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>phone_number</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>email</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>budget</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>current_location</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>preferred_location</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>profession</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>date_of_visit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LLaMA + Prompt 6</td>\n",
       "      <td>buying_timeline_weeks</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_prompt                 entity  precision    recall        f1\n",
       "0  LLaMA + Prompt 6             first_name        1.0  1.000000  1.000000\n",
       "1  LLaMA + Prompt 6              last_name        1.0  1.000000  1.000000\n",
       "2  LLaMA + Prompt 6           phone_number        0.0  0.000000  0.000000\n",
       "3  LLaMA + Prompt 6                  email        1.0  1.000000  1.000000\n",
       "4  LLaMA + Prompt 6                 budget        1.0  0.684211  0.812500\n",
       "5  LLaMA + Prompt 6       current_location        1.0  0.950000  0.974359\n",
       "6  LLaMA + Prompt 6     preferred_location        1.0  1.000000  1.000000\n",
       "7  LLaMA + Prompt 6             profession        0.8  1.000000  0.888889\n",
       "8  LLaMA + Prompt 6          date_of_visit        1.0  0.875000  0.933333\n",
       "9  LLaMA + Prompt 6  buying_timeline_weeks        1.0  0.526316  0.689655"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_prompt6_entities_df = pd.concat([\n",
    "    entity_wise_df(all_metrics[\"llama_prompt6_raw.json\"], \"LLaMA + Prompt 6\")\n",
    "])\n",
    "\n",
    "llama_prompt6_entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "802fe1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_prompt</th>\n",
       "      <th>entity</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>first_name</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>last_name</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>phone_number</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>email</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>budget</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>current_location</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>preferred_location</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>profession</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>date_of_visit</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qwen + Prompt 6</td>\n",
       "      <td>buying_timeline_weeks</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_prompt                 entity  precision    recall        f1\n",
       "0  Qwen + Prompt 6             first_name   1.000000  1.000000  1.000000\n",
       "1  Qwen + Prompt 6              last_name   1.000000  1.000000  1.000000\n",
       "2  Qwen + Prompt 6           phone_number   0.000000  0.000000  0.000000\n",
       "3  Qwen + Prompt 6                  email   1.000000  1.000000  1.000000\n",
       "4  Qwen + Prompt 6                 budget   1.000000  0.894737  0.944444\n",
       "5  Qwen + Prompt 6       current_location   1.000000  0.950000  0.974359\n",
       "6  Qwen + Prompt 6     preferred_location   1.000000  0.800000  0.888889\n",
       "7  Qwen + Prompt 6             profession   0.857143  1.000000  0.923077\n",
       "8  Qwen + Prompt 6          date_of_visit   1.000000  0.875000  0.933333\n",
       "9  Qwen + Prompt 6  buying_timeline_weeks   1.000000  0.473684  0.642857"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_prompt6_entities_df = pd.concat([\n",
    "    entity_wise_df(all_metrics[\"qwen_prompt6_raw.json\"], \"Qwen + Prompt 6\")\n",
    "])\n",
    "\n",
    "qwen_prompt6_entities_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
